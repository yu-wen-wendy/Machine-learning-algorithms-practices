{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NumPy, pandas and time ( No other library/package than the Python 3 standard library, NumPy, pandas and time may be used )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reused functions from assignment 1: data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste functions from Assignment 1 here that you need for this assignment\n",
    "\n",
    "def create_normalization(df, normalizationtype=\"minmax\"):\n",
    "    \"\"\"\n",
    "    Make normalization vectors based on training dataset\n",
    "    \"\"\"\n",
    "    #Hint 1: Basic\n",
    "    #copy the DataFrame first\n",
    "    df_copy = df.copy()\n",
    "    columns = df_copy.columns\n",
    "    normalization = {}\n",
    "    \n",
    "    for i in columns:\n",
    "        dtype = df_copy[i].dtype\n",
    "\n",
    "        #Hint 2: Constratints handling\n",
    "        #do not care about ID or CLASS\n",
    "        if i == \"ID\" or i == \"CLASS\":\n",
    "            continue\n",
    "            \n",
    "        #only care about int and float\n",
    "        if not np.issubdtype(dtype, np.integer) and not np.issubdtype(dtype, np.floating):\n",
    "            continue\n",
    "\n",
    "        if normalizationtype == \"minmax\":\n",
    "            min_value = np.min(df_copy[i])\n",
    "            max_value = np.max(df_copy[i])\n",
    "            df_copy[i] = df_copy[i].apply(lambda x: (x-min_value)/(max_value-min_value))\n",
    "            normalization[i] = (normalizationtype, min_value, max_value)\n",
    "            \n",
    "        elif normalizationtype == \"zscore\":\n",
    "            mean = df_copy[i].mean()\n",
    "            std = df_copy[i].std()\n",
    "            df_copy[i] = df_copy[i].apply(lambda x: (x-mean/std))\n",
    "            normalization[i] = (normalizationtype, mean, std)\n",
    "    \n",
    "    return df_copy, normalization\n",
    "    \n",
    "def apply_normalization(df, normalization):\n",
    "    \"\"\"\n",
    "    Apply training information onto test dataset\n",
    "    \"\"\"\n",
    "    #Hint 1: Basic\n",
    "    #copy the DataFrame first\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    #key: column name\n",
    "    #val: ('method', number1, number2)\n",
    "    for key, val in normalization.items():\n",
    "        normalizationtype = val[0]\n",
    "        \n",
    "        #Hint 2: Constratints handling\n",
    "        #do not care about ID or CLASS\n",
    "        dtype = df_copy[key].dtype\n",
    "        if key == \"ID\" or key == \"CLASS\":\n",
    "            continue\n",
    "            \n",
    "        #only care about int and float\n",
    "        if not np.issubdtype(dtype, np.integer) and not np.issubdtype(dtype, np.floating):\n",
    "            continue\n",
    "            \n",
    "        if normalizationtype == \"minmax\":\n",
    "            min_value = val[1]\n",
    "            max_value = val[2]\n",
    "            df_copy[key] = df_copy[key].apply(lambda x: (x-min_value)/(max_value-min_value))\n",
    "            \n",
    "            #Hint 2: apply strong constraint limit [0,1]\n",
    "            #there are both way and both are working fine\n",
    "            \"\"\"\n",
    "            df_copy.loc[df_copy[key] < 0, key] = 0 \n",
    "            df_copy.loc[df_copy[key] > 1, key] = 1\n",
    "            \"\"\"\n",
    "            df_copy[key] = df_copy[key].clip(0,1)\n",
    "            \n",
    "        elif normalizationtype == \"zscore\":\n",
    "            mean = val[1]\n",
    "            std = val[2]\n",
    "            df_copy[key] = df_copy[key].apply(lambda x: (x-mean/std))\n",
    "            \n",
    "    return df_copy\n",
    "\n",
    "def create_imputation(df):\n",
    "    #Hint 1: Basic\n",
    "    #copy the DataFrame first\n",
    "    df_copy = df.copy()\n",
    "    columns = df_copy.columns\n",
    "    imputation = {}\n",
    "    \n",
    "    for i in columns:\n",
    "        dtype = df_copy[i].dtype\n",
    "\n",
    "        #Hint 2: Constratints handling\n",
    "        #do not care about ID or CLASS\n",
    "        if i == \"ID\" or i == \"CLASS\":\n",
    "            continue\n",
    "        #Case 1: continuous -> use mean\n",
    "        if np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.floating):\n",
    "            #Special case: all values are missing\n",
    "            if np.all(df_copy[i].isnull()):\n",
    "                criteria = 0\n",
    "            #regular case\n",
    "            else:\n",
    "                criteria = df_copy[i].mean()\n",
    "        #case 2: categorical -> use mode\n",
    "        elif hasattr(df_copy[i], 'cat'):\n",
    "            #Special case: all values are missing\n",
    "            if np.all(df_copy[i].isnull()):\n",
    "                criteria = df_copy[i].cat.categories[0]\n",
    "            #regular case\n",
    "            else:\n",
    "                print(df_copy[i][df_copy[i].notnull()])\n",
    "                criteria = df_copy[i].mode()[0] #always return series\n",
    "        #case 3: object case -> cannot apply .cat -> use \"\" when all are missing\n",
    "        #not sure about dtype == \"object\" or else\n",
    "        elif dtype == \"object\":\n",
    "            #Special case: all values are missing\n",
    "            if np.all(df_copy[i].isnull()):\n",
    "                criteria = \"\"\n",
    "            #regular case\n",
    "            else:\n",
    "                criteria = df_copy[i].mode()[0] #always return series\n",
    "        #except object, categorical, numerical -> but there is no case when we load a file\n",
    "        else:\n",
    "            print(dtype)\n",
    "\n",
    "        #apply criteria (use fillna)\n",
    "        df_copy[i] = df_copy[i].fillna(criteria)\n",
    "        #add value into imputation dictionary\n",
    "        imputation[i] = criteria\n",
    "    \n",
    "    return df_copy, imputation\n",
    "\n",
    "def apply_imputation(df, imputation):\n",
    "    #Hint 1: Basic\n",
    "    #copy the DataFrame first\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    #key: column name\n",
    "    #val: imputation value\n",
    "    for key, val in imputation.items():\n",
    "        \n",
    "        #Hint 2: Constratints handling\n",
    "        #do not care about ID or CLASS (safe check when applying!)\n",
    "        if key == \"ID\" or key == \"CLASS\": \n",
    "            continue\n",
    "            \n",
    "        criteria = val\n",
    "        df_copy[key] = df_copy[key].fillna(criteria)\n",
    "            \n",
    "    return df_copy\n",
    "\n",
    "def create_one_hot(df):\n",
    "    #Hint 1: Basic\n",
    "    #copy the DataFrame first\n",
    "    df_copy = df.copy()\n",
    "    columns = df_copy.columns\n",
    "    output = {}\n",
    "    \n",
    "    for i in columns:\n",
    "        dtype = df_copy[i].dtype\n",
    "\n",
    "        #Hint 2: Constratints handling\n",
    "        #do not care about ID or CLASS\n",
    "        if i == \"ID\" or i == \"CLASS\":\n",
    "            continue\n",
    "            \n",
    "        #we only need to care about object and categorical values\n",
    "        #but practically when we load a file, there is no categorical value\n",
    "        #Hint 2 - Case 1: object\n",
    "        if dtype == \"object\":\n",
    "            #change the type into category to make one hot easier\n",
    "            df_copy[i] = df_copy[i].astype(\"category\")\n",
    "        #Hint 2 - Case 2: category\n",
    "        if hasattr(df_copy[i], 'cat'):\n",
    "            cats = df_copy[i].cat.categories\n",
    "            for cat in cats:\n",
    "                #make new column and make type as float\n",
    "                df_copy[i+'-'+cat] = (df_copy[i] == cat).astype(\"float\")\n",
    "            #delete original column\n",
    "            df_copy.drop(i, axis=1, inplace=True)\n",
    "            output[i] = cats\n",
    "    \n",
    "    return df_copy, output\n",
    "\n",
    "def apply_one_hot(df, one_hot):\n",
    "    #Hint 1: Basic\n",
    "    #copy the DataFrame first\n",
    "    df_copy = df.copy()\n",
    "    columns = df_copy.columns\n",
    "    \n",
    "    for i, cats in one_hot.items():\n",
    "        dtype = df_copy[i].dtype\n",
    "\n",
    "        #Hint 2: Constratints handling\n",
    "        #do not care about ID or CLASS\n",
    "        if i == \"ID\" or i == \"CLASS\":\n",
    "            continue\n",
    "            \n",
    "        #Hint 2 - Case 1: category\n",
    "        if dtype == \"object\":\n",
    "            #change the type into category to make one hot easier\n",
    "            df_copy[i] = df_copy[i].astype(\"category\")\n",
    "        if hasattr(df_copy[i], 'cat'):\n",
    "            for cat in cats:\n",
    "                #make new column and make type as float\n",
    "                df_copy[i+'-'+cat] = (df_copy[i] == cat).astype(\"float\")\n",
    "            #delete original column\n",
    "            df_copy.drop(i, axis=1, inplace=True)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def create_bins(df, nobins=10, bintype=\"equal-width\"):\n",
    "    #Hint 1: Basic\n",
    "    #copy the DataFrame first\n",
    "    df_copy = df.copy()\n",
    "    columns = df_copy.columns\n",
    "    binning = {}\n",
    "    \n",
    "    for i in columns:\n",
    "        dtype = df_copy[i].dtype\n",
    "\n",
    "        #Hint 2: Constratints handling\n",
    "        #do not care about ID or CLASS\n",
    "        if i == \"ID\" or i == \"CLASS\":\n",
    "            continue\n",
    "        #only care about int and float\n",
    "        if not np.issubdtype(dtype, np.integer) and not np.issubdtype(dtype, np.floating):\n",
    "            continue\n",
    "        \n",
    "        #Hint 3 - Case 1: equal width -> cut\n",
    "        if bintype == \"equal-width\":\n",
    "            res, bins = pd.cut(df_copy[i], bins=nobins, labels=False, retbins=True, duplicates=\"drop\")\n",
    "        #Hint 3 - Case 2: equal size -> qcut\n",
    "        elif bintype == \"equal-size\":\n",
    "            res, bins = pd.qcut(df_copy[i], q=nobins, labels=False, retbins=True, duplicates=\"drop\")\n",
    "            \n",
    "        #apply res\n",
    "        df_copy[i] = res\n",
    "        \n",
    "        #Hint 4 - Set column to be of type \"category\"\n",
    "        df_copy[i] = df_copy[i].astype(\"category\")\n",
    "        \n",
    "        #Hint 5 - set the categories as a number of bins\n",
    "        df_copy[i] = df_copy[i].cat.set_categories(list(range(len(bins))))\n",
    "        \n",
    "        #Hint 6 - set first and last value\n",
    "        bins[0] = -np.inf\n",
    "        bins[-1] = np.inf\n",
    "        \n",
    "        #set bins on the output\n",
    "        binning[i] = bins\n",
    "    \n",
    "    return df_copy, binning\n",
    "\n",
    "def apply_bins(df, binning):\n",
    "    #Hint 1: Basic\n",
    "    #copy the DataFrame first\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for key, val in binning.items():\n",
    "        dtype = df_copy[key].dtype\n",
    "        #Hint 2: Constratints handling\n",
    "        \n",
    "        #do not care about ID or CLASS (safe check when applying!)\n",
    "        if key == \"ID\" or key == \"CLASS\":\n",
    "            continue\n",
    "        #only care about int and float\n",
    "        if not np.issubdtype(dtype, np.integer) and not np.issubdtype(dtype, np.floating):\n",
    "            continue\n",
    "        \n",
    "        #Hint 2\n",
    "        res = pd.cut(df_copy[key], bins=val, labels=False, duplicates=\"drop\")\n",
    "        df_copy[key] = res\n",
    "        \n",
    "        #Hint 3 - Set column to be of type \"category\"\n",
    "        df_copy[key] = df_copy[key].astype(\"category\")\n",
    "\n",
    "        #Hint 4 - set the categories as a number of nobins\n",
    "        df_copy[key] = df_copy[key].cat.set_categories(list(range(len(val))))\n",
    "        \n",
    "    return df_copy\n",
    "\n",
    "def brier_score(df, correctlabels):\n",
    "    #setting dictionary\n",
    "    correctdict = {}\n",
    "    brier_score = 0\n",
    "    \n",
    "    for i in correctlabels:\n",
    "        if i not in correctdict.keys():\n",
    "            correctdict[i] = [0]\n",
    "    \n",
    "    correctdict = pd.DataFrame(correctdict)\n",
    "\n",
    "    #loop number of samples\n",
    "    for cnt, val in enumerate(correctlabels):\n",
    "        #initialize 0 again\n",
    "        correctdict.iloc[0] = 0\n",
    "        #only correct one goes to 1\n",
    "        correctdict[val] = 1\n",
    "        #get single row in a prediction\n",
    "        row = df.iloc[cnt]\n",
    "        #calculate score\n",
    "        score = np.sum(np.square(row-correctdict), axis=1)\n",
    "        brier_score += score\n",
    "    \n",
    "    brier_score /= len(correctlabels)\n",
    "    return brier_score[0]\n",
    "\n",
    "import collections\n",
    "\n",
    "def auc(df, correctlabels):\n",
    "    #make labels\n",
    "    labels = set(correctlabels)\n",
    "    \n",
    "    #assignment 2 fix\n",
    "    correctlabels = correctlabels.tolist()\n",
    "    \n",
    "    counts = [] #for weighted sum (count of class in true population)\n",
    "    aucs = [] #auc score for each count\n",
    "    \n",
    "    #calculate TP/FP for each label\n",
    "    for label in labels:\n",
    "        #make scores \n",
    "        tot_tp = 0\n",
    "        tot_fp = 0\n",
    "        scores = {}\n",
    "        \n",
    "        for idx, val in enumerate(correctlabels):\n",
    "            if df.iloc[idx][label] not in scores.keys():\n",
    "                scores[df.iloc[idx][label]] = [0, 0]\n",
    "            if val == label:\n",
    "                scores[df.iloc[idx][label]][0] += 1\n",
    "                tot_tp += 1\n",
    "            else:\n",
    "                scores[df.iloc[idx][label]][1] += 1\n",
    "                tot_fp += 1\n",
    "         \n",
    "        #Descending sort by its score\n",
    "        scores = collections.OrderedDict(sorted(scores.items(), reverse=True))\n",
    "        \n",
    "        #GET AUC score\n",
    "        auc_sub = 0\n",
    "        cov_tp = 0\n",
    "        \n",
    "        for key, val in scores.items():\n",
    "            tp_rate = val[0]\n",
    "            fp_rate = val[1]\n",
    "            if fp_rate == 0:\n",
    "                cov_tp += tp_rate\n",
    "            elif tp_rate == 0:\n",
    "                auc_sub += (cov_tp/tot_tp) * (fp_rate/tot_fp)\n",
    "            else:\n",
    "                auc_sub += (cov_tp/tot_tp)*(fp_rate/tot_fp) + ((tp_rate/tot_tp)*(fp_rate/tot_fp))/2\n",
    "                cov_tp += tp_rate\n",
    "        \n",
    "        #apply proportion\n",
    "        counts.append(correctlabels.count(label))\n",
    "        aucs.append(auc_sub)\n",
    "        \n",
    "    auc = np.array(aucs).dot(np.array(counts))/len(correctlabels)\n",
    "\n",
    "    return auc\n",
    "\n",
    "def accuracy(df, correctlabels):\n",
    "    df_copy = df.copy()\n",
    "    pred = np.empty(len(correctlabels))\n",
    "    \n",
    "    df_max = df_copy.max(axis=1) #find the highest value in each row to compare later\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        df_tmp = df_copy.iloc[i:i+1]\n",
    "        for col in df_tmp.columns:\n",
    "            if(df_tmp[col] >= df_max[i:i+1]).bool(): \n",
    "                pred[i] = col\n",
    "                \"\"\"\n",
    "                if break enabled, will pick the first option, \n",
    "                else, will leave the last option that equals the highest value, \n",
    "                can be randomized with an if and random function\n",
    "                \"\"\"\n",
    "                #1. random mode\n",
    "                #if np.random.choice([True, False]): break \n",
    "                \n",
    "                #2. picking first one mode\n",
    "                break\n",
    "                \n",
    "    numbercorrect = np.sum(np.array(correctlabels) == pred)\n",
    "    return numbercorrect/len(correctlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the class kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class kNN with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# imputation, normalization, one_hot, labels, training_labels, training_data\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# normalizationtype: \"minmax\" (default) or \"zscore\"\n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.imputation should be an imputation mapping (see Assignment 1) from df\n",
    "# self.normalization should be a normalization mapping (see Assignment 1), using normalizationtype from the imputed df\n",
    "# self.one_hot should be a one-hot mapping (see Assignment 1; can be excluded if this function was not completed)\n",
    "# self.training_labels should be a pandas series corresponding to the \"CLASS\" column, set to be of type \"category\" \n",
    "# self.labels should be the categories of the previous series\n",
    "# self.training_data should be the values (an ndarray) of the transformed dataframe, i.e., after employing imputation, \n",
    "# normalization, and possibly one-hot encoding, and also after removing the \"CLASS\" and \"ID\" columns \n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# k: an integer >= 1 (default = 5)\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "#              predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "#              are estimated by the relative class frequencies in the set of class labels from the k nearest \n",
    "#              (with respect to Euclidean distance) neighbors in training_data\n",
    "#\n",
    "# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation, normalization and (possibly) one-hot\n",
    "# Hint 2: Get the numerical values (as an ndarray) from the resulting dataframe and iterate over the rows \n",
    "#         calling some sub-function, e.g., get_nearest_neighbor_predictions(x_test,k), which for a test row\n",
    "#         (numerical input feature values) finds the k nearest neighbors and calculate the class probabilities.\n",
    "# Hint 3: This sub-function may first find the distances to all training instances, e.g., pairs consisting of\n",
    "#         training instance index and distance, and then sort them according to distance, and then (using the indexes\n",
    "#         of the k closest instances) find the corresponding labels and calculate the relative class frequencies\n",
    "class kNN():\n",
    "    def __init__(self):\n",
    "        self.imputation = None\n",
    "        self.normalization = None\n",
    "        self.one_hot = None\n",
    "        self.labels = None\n",
    "        self.training_labels = None\n",
    "        self.training_data = None\n",
    "        \n",
    "    def fit(self, df, normalizationtype=\"minmax\"):\n",
    "        #we will keep making the training data after copying original df\n",
    "        self.training_data = df.copy()\n",
    "        #1: imputation for null/na values\n",
    "        self.training_data, self.imputation = create_imputation(self.training_data)\n",
    "        #2: normalization\n",
    "        self.training_data, self.normalization = create_normalization(self.training_data, normalizationtype)\n",
    "        #3: one hot encoding\n",
    "        self.training_data, self.one_hot = create_one_hot(self.training_data)\n",
    "        #4: set training data as a category type\n",
    "        self.training_labels = df[\"CLASS\"].astype('category')\n",
    "        #5: extract labels\n",
    "        self.labels = self.training_labels.cat.categories\n",
    "        #6: drop ID and CLASS column to make final training data\n",
    "        #it is better to check it seperately because in some unknown dataset there can be only one among ID and CLASS\n",
    "        if \"ID\" in self.training_data.columns:\n",
    "            self.training_data.drop([\"ID\"], inplace=True, axis=1)\n",
    "        if \"CLASS\" in self.training_data.columns:\n",
    "            self.training_data.drop([\"CLASS\"], inplace=True, axis=1)\n",
    "    \n",
    "    def get_nearest_neighbor_predictions(self, x_test, k):\n",
    "        \"\"\"\n",
    "        get k nearest row indices based on euclidian\n",
    "        \"\"\"\n",
    "        length_applied = np.sum(np.square(self.training_data - x_test), axis=1)\n",
    "        #get k labels based on sorted result\n",
    "        labels = self.training_labels[length_applied.sort_values().iloc[:k].index]\n",
    "        #always return series\n",
    "        label = labels.value_counts()\n",
    "        #make it as dataframe and return\n",
    "        return label/k\n",
    "        \n",
    "    def predict(self, df, k=5):\n",
    "        #always copy dataset\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        #Hint 1: drop id and class\n",
    "        #it is better to check it seperately because in some unknown dataset there can be only one among ID and CLASS\n",
    "        if \"ID\" in df_copy.columns:\n",
    "            df_copy.drop([\"ID\"], inplace=True, axis=1)\n",
    "        if \"CLASS\" in df_copy.columns:\n",
    "            df_copy.drop([\"CLASS\"], inplace=True, axis=1)\n",
    "        \n",
    "        #Hint 1: apply imputation, normalization, one hot encoding\n",
    "        df_copy = apply_imputation(df_copy, self.imputation)\n",
    "        df_copy = apply_normalization(df_copy, self.normalization)\n",
    "        df_copy = apply_one_hot(df_copy, self.one_hot)\n",
    "        \n",
    "        #Hint 2: sort only numerical data\n",
    "        df_numeric = df_copy._get_numeric_data()\n",
    "        \n",
    "        #Hint 2: find k nearest data\n",
    "        nearests = df_numeric.apply(self.get_nearest_neighbor_predictions, axis=1, k=k)\n",
    "        \n",
    "        return nearests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.01 s.\n",
      "Testing time (k=1): 0.51 s.\n",
      "Testing time (k=3): 0.44 s.\n",
      "Testing time (k=5): 0.43 s.\n",
      "Testing time (k=7): 0.42 s.\n",
      "Testing time (k=9): 0.42 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.810350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.663551</td>\n",
       "      <td>0.488058</td>\n",
       "      <td>0.815859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.471028</td>\n",
       "      <td>0.833843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.471867</td>\n",
       "      <td>0.833481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.482981</td>\n",
       "      <td>0.827727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Brier score       AUC\n",
       "1  0.747664     0.504673  0.810350\n",
       "3  0.663551     0.488058  0.815859\n",
       "5  0.579439     0.471028  0.833843\n",
       "7  0.598131     0.471867  0.833481\n",
       "9  0.616822     0.482981  0.827727"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "knn_model = kNN()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "knn_model.fit(glass_train_df)\n",
    "print(\"Training time: {0:.2f} s.\".format(time.perf_counter()-t0))\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "k_values = [1,3,5,7,9]\n",
    "results = np.empty((len(k_values),3))\n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = knn_model.predict(glass_test_df,k=k_values[i])\n",
    "    print(\"Testing time (k={0}): {1:.2f} s.\".format(k_values[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=k_values,columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set (k=1): 1.00\n",
      "AUC on training set (k=1): 1.00\n",
      "Brier score on training set (k=1): 0.00\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "predictions = knn_model.predict(glass_train_df,k=1)\n",
    "print(\"Accuracy on training set (k=1): {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set (k=1): {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set (k=1): {0:.2f}\".format(brier_score(predictions,train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything works fine. We followed all constraints and results were same.\n",
    "\n",
    "- We modified one thing in auc function from assignment 1.\n",
    " - because now the correct label is RangeIndex type but in assignment 1, it was just normal python list. \n",
    " - Therefore we put this line in auc function to make it work well (our function was based on python list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#assignment 2 fix\n",
    "correctlabels = correctlabels.tolist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the class NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class NaiveBayes with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, class_priors, feature_class_value_counts, feature_class_counts\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# nobins: no. of bins (default = 10)\n",
    "# bintype: either \"equal-width\" (default) or \"equal-size\" \n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.binning should be a discretization mapping (see Assignment 1) from df\n",
    "# self.class_priors should be a mapping (dictionary) from the labels (categories) of the \"CLASS\" column of df,\n",
    "# to the relative frequencies of the labels\n",
    "# self.feature_class_value_counts should be a mapping from a feature (column name) to another mapping, which\n",
    "# given a feature value and class label provides the number of training instances with this specific combination\n",
    "# self.feature_class_counts should me a mapping from the feature (column name) and class label to the number of\n",
    "# training instances with this specific class label and any (non-missing) value for the feature\n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Hint 1: feature_class_value_counts can be a dictionary, which given a feature f returns a mapping obtained \n",
    "#         by pandas groupby and size (see lecture slides), which given a feature value v and class label c \n",
    "#         returns the number of instances, e.g., using get((c,v),0)\n",
    "#\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "# predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "# are estimated by the naive approximation of Bayes rule (see lecture slides)\n",
    "#\n",
    "# Hint 1: First apply discretization\n",
    "# Hint 2: Iterating over either columns or rows, and for each possible class label, calculate the relative\n",
    "#         frequency of the observed feature value given the class (using feature_class_value_counts and \n",
    "#         feature_class_counts) \n",
    "# Hint 3: Calculate the non-normalized estimated class probabilities by multiplying the class priors to the\n",
    "#         product of the relative frequencies\n",
    "# Hint 4: Normalize the probabilities by dividing by the sum of the non-normalized probabilities; in case\n",
    "#         this sum is zero, then set the probabilities to the class priors\n",
    "class NaiveBayes():\n",
    "    def __init__(self):\n",
    "        self.binning = None\n",
    "        self.class_priors = None\n",
    "        self.feature_class_value_counts = None\n",
    "        self.feature_class_counts = None\n",
    "    \n",
    "    \n",
    "    def fit(self, df, nobins=10, bintype=\"equal-width\"):\n",
    "        \n",
    "        #Hint 1. Applying discretization\n",
    "        df_copy, self.binning = create_bins(df, nobins=nobins, bintype=bintype)\n",
    "        self.class_priors = df_copy[\"CLASS\"].value_counts()/len(df_copy[\"CLASS\"])\n",
    "        \n",
    "        #1. get class unique values\n",
    "        classes = df_copy[\"CLASS\"].unique()\n",
    "        \n",
    "        self.feature_class_value_counts = {}\n",
    "        self.feature_class_counts = {}\n",
    "        \n",
    "        #Hint 2: for each class, get each value sing feature_class_value_counts and feature_class_counts\n",
    "        for f in df_copy.columns:\n",
    "            if f == \"ID\" or f == \"CLASS\":\n",
    "                continue\n",
    "            \n",
    "            self.feature_class_value_counts[f] = {}\n",
    "            self.feature_class_counts[f] = {}\n",
    "            for c in classes:\n",
    "                temp = 0\n",
    "                for v in df_copy[f].unique():\n",
    "                    #self.feature_class_value_counts[f][v] = {}\n",
    "                    sumclass = ((df_copy[\"CLASS\"] == c) & (df_copy[f] == v)).sum()\n",
    "                    self.feature_class_value_counts[f][(c,v)] = sumclass\n",
    "                    temp += sumclass\n",
    "                \n",
    "                #The sub dictionary will be the same for all class! (because there is only one class column)\n",
    "                self.feature_class_counts[f][c] = temp\n",
    "                \n",
    "        self.feature_class_value_counts = pd.DataFrame(self.feature_class_value_counts)\n",
    "        self.feature_class_counts = pd.DataFrame(self.feature_class_counts)\n",
    "\n",
    "        \n",
    "    def get_naive_bayes_probability(self, row, classes):\n",
    "        probs = []\n",
    "        #looping through the class\n",
    "        for c in classes:\n",
    "            rela_freq = 1\n",
    "\n",
    "            #Hint 3: calculate Naive Bayes probability for each column\n",
    "            for f in row.index:\n",
    "                if f == \"ID\" or f == \"CLASS\":\n",
    "                    continue\n",
    "                v = row[f]\n",
    "\n",
    "                classprob = self.feature_class_counts[f][c]\n",
    "                valueprob = self.feature_class_value_counts[f][c,v]\n",
    "                rela_freq *= valueprob/classprob\n",
    "\n",
    "            probs.append(rela_freq*self.class_priors[c])\n",
    "        \n",
    "        #Hint 4: post processing (normalization)\n",
    "        probs = pd.Series(probs, index = classes)\n",
    "        \n",
    "        #in case the probability sum is zero, then set the probabilities to the class priors\n",
    "        if probs.sum() != 0:\n",
    "            probs = probs / probs.sum()\n",
    "        else:\n",
    "            probs = self.class_priors\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, df):\n",
    "        #Hint 1: apply binning (discretization)\n",
    "        df_copy = apply_bins(df, self.binning)\n",
    "        #get class lists\n",
    "        classes = df_copy[\"CLASS\"].unique()\n",
    "        #Hint 2: find 2~3 using apply for performance\n",
    "        probs = df_copy.apply(self.get_naive_bayes_probability, axis=1, classes=classes)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (3, 'equal-width'): 0.13 s.\n",
      "Testing time (3, 'equal-width'): 0.39 s.\n",
      "Training time (3, 'equal-size'): 0.13 s.\n",
      "Testing time (3, 'equal-size'): 0.34 s.\n",
      "Training time (5, 'equal-width'): 0.19 s.\n",
      "Testing time (5, 'equal-width'): 0.37 s.\n",
      "Training time (5, 'equal-size'): 0.17 s.\n",
      "Testing time (5, 'equal-size'): 0.34 s.\n",
      "Training time (10, 'equal-width'): 0.33 s.\n",
      "Testing time (10, 'equal-width'): 0.36 s.\n",
      "Training time (10, 'equal-size'): 0.31 s.\n",
      "Testing time (10, 'equal-size'): 0.37 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.622116</td>\n",
       "      <td>0.724335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.554782</td>\n",
       "      <td>0.780163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.551101</td>\n",
       "      <td>0.771688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.581556</td>\n",
       "      <td>0.796675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>equal-width</th>\n",
       "      <td>0.654206</td>\n",
       "      <td>0.527569</td>\n",
       "      <td>0.812887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal-size</th>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.741668</td>\n",
       "      <td>0.751165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Accuracy  Brier score       AUC\n",
       "3  equal-width  0.616822     0.622116  0.724335\n",
       "   equal-size   0.607477     0.554782  0.780163\n",
       "5  equal-width  0.644860     0.551101  0.771688\n",
       "   equal-size   0.598131     0.581556  0.796675\n",
       "10 equal-width  0.654206     0.527569  0.812887\n",
       "   equal-size   0.588785     0.741668  0.751165"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "nb_model = NaiveBayes()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "nobins_values = [3,5,10]\n",
    "bintype_values = [\"equal-width\",\"equal-size\"]\n",
    "parameters = [(nobins,bintype) for nobins in nobins_values for bintype in bintype_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    t0 = time.perf_counter()\n",
    "    nb_model.fit(glass_train_df,nobins=parameters[i][0],bintype=parameters[i][1])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = nb_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([nobins_values,bintype_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.85\n",
      "AUC on training set: 0.97\n",
      "Brier score on training set: 0.23\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "nb_model.fit(glass_train_df)\n",
    "predictions = nb_model.predict(glass_train_df)\n",
    "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc.\n",
    "Everything works fine. We followed all constraints and results were same.\n",
    "\n",
    "- Regarding hint 1 for Fit function, we didn't use the GroupBy() function to calculate the feature_class_value_counts and feature_class_counts. \n",
    "- Instead, we used pandas value_counts() function to acheive the same results, because it is neat :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
