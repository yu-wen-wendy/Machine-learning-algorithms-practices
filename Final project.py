# -*- coding: utf-8 -*-
"""Assg 4 GS Share - Comments Deleted

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V5vXFbR8Bpxs_4sWhTFENDGr_2Phe0uL

# ID2214 Assignment 4 Group no. [10]
### Project members: 
[Heeje Lee, heeje@kth.se]
[Victor Castillo, vcas@kth.se]
[Yu-Wen Huang, ywhu@kth.se]
[Rachhek Shrestha, rachhek@kth.se]

### Project Description:
In our project, we implemented a user set parameter comparison and included a grid-search heuristic algorithm to find best situation for each library. Four well-known ensemble tree and boosting frameworks (SKlearn Gradient Boosting Decision Tree, SKlearn Random Forest, Microsoftâ€™s Light Gradient Boosting Method, XGBoost) were chosen to compare by 6 performance metrics: accuracy, precision, sensitivity, specificity, F1-score, and executing time. We calculate a weighted sum of score of the performance metrics, which the weights are input by the users. The results from these libraries are then collected and the Rank Calculator module carries out the weighted sum to rank the algorithms from best to worst and shows the executing time. This will help data scientists to accelerate the construction of a model, reduce pre-production time and make better decisions.


### Program Sequences:
**Input Handler**(Function : getInput)
  Handles the users preferences of weight of performance metrics and algorithm parameters
  
**Data Preprocess**:(Function: dataPreprocess)
  Uses the inputs from Input Handler to prepare the data before processing

**Parameter Abstraction Module**(Function : parameterAbstract)
  Prepares the library specific parameters that will be used in the library call
 
**Library Call** (Function: CallLibrary, CallXGboost, CallLGBM, CallSKGBDT, CallSKRF )
  Runs all the libraries under investigation and returns the values of performance metrics

**Rank Calculator** (Function: calculateRank)
  Gets the output from CallLibrary function and calculates the weighted sum to rank the algorithm from best to worst
  
  **Main function** (Function: __main__ )
  Function that brings together all the data preprocessing, parameter abstraction, library call and calculates the rank

## Libraries:

We should provide link for each libraries
1. Numpy : http://www.numpy.org/
2. Pandas: https://pandas.pydata.org/
3. Sklearn : https://scikit-learn.org/stable/
4. Matplotlib : https://matplotlib.org/
5. LightGBM : https://lightgbm.readthedocs.io/en/latest/#
6. XGBoost : https://xgboost.readthedocs.io/en/latest/
7. Plotly :  https://plot.ly/python/
8. Time : https://docs.python.org/3/library/time.html
9. Warnings : https://docs.python.org/3/library/warnings.html
"""

import numpy as np
import pandas as pd
import sklearn
import datetime
import matplotlib.pyplot as plt
import lightgbm as lgb
import xgboost as xgb
import time
from sklearn import preprocessing 
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import Imputer
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
import warnings
import plotly
import plotly.graph_objs as go
plotly.tools.set_credentials_file(username='rachhek', api_key='KhsONk54AGKRTmFOkKHV')

"""## Codes

##dataPreprocess
"""

"""
This function preprocesses the data.
Carries out Imputation, Encoding, Normalization, Kfolds and Data Split
"""
def dataPreprocess(loc, labelName, Centralvalue, 
                   normalizationtype, useKfolds,
                   trainSize, useGridSearch):

  dataset = pd.read_csv(loc)

  dataset = dataset.rename(columns = {labelName:'CLASS'})    

  X = dataset.drop(columns=["CLASS"])
  y = dataset["CLASS"].astype('category').values

  #Separate num. columns and cat. columns in X dataset
  obj_types = np.array(X.dtypes == np.object)
  num_types = np.array((X.dtypes == np.float64) | (X.dtypes == np.int64))
  X_cat = X.loc[:, obj_types]
  X_nums = X.loc[:, num_types]

  # Taking care of missing data
  from sklearn.preprocessing import Imputer

  X_cat = X_cat.fillna(X_cat.mode().iloc[0])

  if Centralvalue == "mean":
      imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
  elif Centralvalue == "median":
      imputer = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0)
  elif Centralvalue == "mode":
      imputer = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)

  if X_nums.empty is not True:    
    X_nums = imputer.fit_transform(X_nums)

  # Encoding categorical data
  from sklearn.preprocessing import LabelEncoder,OneHotEncoder
  label_encoder = LabelEncoder()
  X_cat = X_cat.apply(label_encoder.fit_transform)
  
  y,class_names = pd.factorize(y)
  
  if len(X_cat) > 0:
    X = np.concatenate((X_cat, X_nums), axis=1)
  else:
    X = X_nums
  y = np.array(y)
  
  #Kfolds method
  if useKfolds >1 and useGridSearch == 0:
    #afterfolds = folds(pd.DataFrame(X), nofolds=useKfolds)
    from sklearn.model_selection import KFold

    kf = KFold(n_splits=useKfolds)
    folds = kf.split(X=X)

    #Feature Scaling in each fold
    if normalizationtype == "minmax":    
      sc = preprocessing.StandardScaler()

    elif normalizationtype == "zscore":
      sc = preprocessing.MinMaxScaler()

    normfolds = []
    for train_idx, test_idx in folds:
      X_train = X[train_idx,:]
      X_test = X[test_idx,:]
      y_train = y[train_idx]
      y_test = y[test_idx]

      X_train = sc.fit_transform(X_train)
      X_test = sc.transform(X_test)
      normfolds.append([X_train, X_test, y_train, y_test])

    return normfolds

  #Split method 
  elif (useKfolds <= 1 and useGridSearch == 0) or (useKfolds >1 and useGridSearch == 1) or (useKfolds <=1 and useGridSearch == 1):

    # Splitting the dataset into the Training set and Test set
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-trainSize, random_state = 925)

    # Feature Scaling    
    if normalizationtype == "minmax":
      sc = preprocessing.MinMaxScaler()
      X_train = sc.fit_transform(X_train)
      X_test = sc.transform(X_test)

    elif normalizationtype == "zscore":
      sc = preprocessing.StandardScaler()
      X_train = sc.fit_transform(X_train)
      X_test = sc.transform(X_test)
            
    return [(X_train), (X_test), (y_train), (y_test)]

"""##parameterAbstract"""

def parameterAbstract(maxDepth, iteration, learningRate, 
                      maxLeafnode, noTree, minData, useGPU):
    """
    Convert input parameters into suitable format for each libraries
    
    Input
     - parameters
      - maxDepth
      - iteration
      - learningRate
      - maxLeafnode
      - noTree
      - minData
      - useGpu
     
    Output
     - output: dictionary for each libraries having parameter name for each library as a key
     
    """
   
    
    output = {
        "XGBoost": {"max_depth": maxDepth, 
                    "num_boost_round": iteration, 
                    "eta": learningRate, 
                    "maxLeafnode": maxLeafnode, #max_leaf_nodes
                    "ntree_limit": noTree, 
                    "min_child_weight": minData
                   },
        "LGBM": {"max_depth": maxDepth,
                 "iteration": iteration, #n_estimators
                 "learning_rate": learningRate, 
                 "num_leaves": maxLeafnode, #Note: default = 31, type = int, aliases: num_leaf, max_leaves, max_leaf, constraints: num_leaves > 1 max number of leaves in one tree
                 "noTree": noTree,
                 "min_data_in_leaf": minData
                },
        "SKGBDT": {"max_depth": maxDepth, 
                   "n_estimators": iteration, 
                   "learning_rate": learningRate, #Note: The parameter learning_rate strongly interacts with the parameter n_estimators, the number of weak learners to fit. Smaller values of learning_rate require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of learning_rate favor better test error. [HTF2009] recommend to set the learning rate to a small constant (e.g. learning_rate <= 0.1) and choose n_estimators by early stopping. https://scikit-learn.org/stable/modules/ensemble.html
                   "max_leaf_nodes": maxLeafnode, 
                   "n_estimators": noTree

                  },
        "SKRF": {"max_depth": maxDepth, 
                 "n_estimators": iteration,
                 "max_leaf_nodes": maxLeafnode, 
                 "n_estimators": noTree
                }
    }
    
    return output

"""##callLibrary"""

#Caller functions for each library
#
#callLibrary
#callXGBoost, callLGBM, callSKGBDT, callSKRF
def callLibrary(parameters, data, useKfolds, useGridSearch):
    """
    Call each library using parameters from parameterAbstract
    - Input
      - parameters: dictionary containing parameters needed for each libraries
        {XGBoost, LGBM, SKGBDT, SKRF}
      - data
      - useKfolds = True/False
      
    """
    
    outputXGboost = {
       'accuracy':0,
       'precision':0,
       'specificity':0,
       'f1score':0,
       'sensitivity':0,
       'Time':0
    }
    outputLGBM = {
       'accuracy':0,
       'precision':0,
       'specificity':0,
       'f1score':0,
       'sensitivity':0,
       'Time':0
    }
    outputSKGBDT = {
       'accuracy':0,
       'precision':0,
       'specificity':0,
       'f1score':0,
       'sensitivity':0,
       'Time':0
    }
    outputSKRF = {
       'accuracy':0,
       'precision':0,
       'specificity':0,
       'f1score':0,
       'sensitivity':0,
       'Time':0
    }
    
    outputXGboost = callXGBoost(**parameters["XGBoost"], data=data, useKfolds=useKfolds)
    outputLGBM = callLGBM(**parameters["LGBM"], data=data, useKfolds=useKfolds)
    outputSKGBDT = callSKGBDT(**parameters["SKGBDT"], data=data, useKfolds=useKfolds)
    outputSKRF = callSKRF(**parameters["SKRF"], data=data, useKfolds=useKfolds)
    
    output = {
        "XGBoost": outputXGboost,
        "LGBM": outputLGBM,
        "SKGBDT": outputSKGBDT,
        "SKRF": outputSKRF
    }
    
    return output

"""##callXGBoost"""

def callXGBoost(max_depth, num_boost_round, eta, 
                maxLeafnode, ntree_limit, min_child_weight, data, useKfolds):
  """
  - Input: parameters needed for XGBoost

  - Output: performance metrics
   - Accuracy
   - Prediction
   - Specificity
   - F1 Score
   - Sensitivity
   - Time
  """
  if useKfolds >1:
    accuracy=[]
    precision=[]
    specificity=[]
    F1Score=[]
    Sensitivity=[]
    Time=[]
    
    for i in range(useKfolds):
      #) Receive the "data" 
      X_train = pd.DataFrame(data[i][0])
      X_test = pd.DataFrame(data[i][1])
      y_train = pd.DataFrame(data[i][2])
      y_test = pd.DataFrame(data[i][3])
            
      
      #) Turn data into XGB DMatrix
      d_train = xgb.DMatrix(X_train, label=y_train)
      d_test = xgb.DMatrix(X_test, label=y_test)

      t0 = time.perf_counter()
      
      #) Configure Parameters for XGBoost
      # specify parameters via map or direct assignment
      param = {
          #Parameters passed from user
          'max_depth': max_depth,  # the maximum depth of each tree
          'eta': eta, 
          'silent': 1  # logging mode - quiet
      }
      
      #Parameters Dependent on the dataset
      #'objective': 'binary:hinge' #'multi:softprob',  # 'binary:logistic'
      if len(y_train[0].unique()) == 2:
        target_type = 'binary:logistic' 
        param['objective'] = target_type
      elif len(y_train[0].unique()) > 2:
        target_type = 'multi:softprob'
        param['objective'] = target_type
        param['num_class'] = len(y_train[0].unique())

      
      #Parameters passed from user
      num_round = num_boost_round
      
      #/* END IF USING SINGLE PARAMETERS
      
      #) Train the XGB Model
      bst = xgb.train(param, d_train, num_round) #enable if no watchlist #classifier

      #) Predict testing data classes, store best predictions in array
      #if 'binary:hinge', y_pred = preds, if 'multi:softprob', it's probabilistic
      if target_type == 'binary:hinge':
        y_pred = bst.predict(d_test)
      elif target_type == 'binary:logistic':
        y_pred = bst.predict(d_test)
        for idx,val in enumerate(y_pred):
          if y_pred[idx] >= .5:
            y_pred[idx] = 1
          else:
            y_pred[idx] = 0
      elif target_type == 'multi:softprob':
        preds = bst.predict(d_test)
        y_pred = np.asarray([np.argmax(line) for line in preds])
      
      # Speed
      speed =np.around(time.perf_counter()-t0, decimals=3)

      # confusion_matrix  
      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
      specificity_fold = tn/(tn+fp)
      
      # evaluation
      accuracy.append(accuracy_score(y_test, y_pred))
      precision.append(metrics.precision_score(y_test, y_pred, average='macro'))
      specificity.append(specificity_fold)
      F1Score.append(metrics.f1_score(y_test, y_pred, average='macro'))
      Sensitivity.append(metrics.recall_score(y_test, y_pred, average='macro'))
      Time.append(speed)
        
    scoring = {'accuracy':np.mean(accuracy),
               'precision':np.mean(precision),
               'specificity':np.mean(specificity),
               'f1score':np.mean(F1Score),
               'sensitivity':np.mean(Sensitivity),
               'Time':np.mean(speed)}

    return scoring

  else:
    X_train = pd.DataFrame(data[0])
    X_test = pd.DataFrame(data[1])
    y_train = pd.DataFrame(data[2])
    y_test = pd.DataFrame(data[3])
    
    #) Turn data into XGB DMatrix
    d_train = xgb.DMatrix(X_train, label=y_train)
    d_test = xgb.DMatrix(X_test, label=y_test)
    
    t0 = time.perf_counter()
    #*/ IF USING SINGLE USER-DEFINED PARAMETERS
    #) Configure Parameters for XGBoost
    # specify parameters via map or direct assignment
    param = {
        'max_depth': max_depth,   
        'eta': eta,  
        'silent': 1        
    }
    
    #'objective': 'binary:hinge' #'multi:softprob',  # 'binary:logistic' for binary case. error evaluation for multiclass training or others.
    #Al algs will suffer from this problem, . Note 2: GS can do this automagically!
    if len(y_train[0].unique()) == 2:
      target_type = 'binary:logistic' #'binary:hinge'
      param['objective'] = target_type
    elif len(y_train[0].unique())> 2:
      target_type = 'multi:softprob'
      param['objective'] = target_type
      param['num_class'] = len(y_train[0].unique())
    
    num_round = num_boost_round
    #/* END IF USING SINGLE PARAMETERS

    #) Train the XGB Model
    bst = xgb.train(param, d_train, num_round) #enable if no watchlist #classifier

    #) Predict testing data classes, store best predictions in array
    #if 'binary:hinge', y_pred = preds, if 'multi:softprob', it's probabilistic
    if target_type == 'binary:hinge':
      y_pred = bst.predict(d_test)
    elif target_type == 'binary:logistic':
      y_pred = bst.predict(d_test)
      for idx,val in enumerate(y_pred):
        if y_pred[idx] >= .5:
          y_pred[idx] = 1
        else:
          y_pred[idx] = 0
    elif target_type == 'multi:softprob':
      preds = bst.predict(d_test)
      y_pred = np.asarray([np.argmax(line) for line in preds]) 
    
    # Speed
    speed =np.around(time.perf_counter()-t0, decimals=3)
    
    # Confusion_matrix
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel() 
    specificity = tn/(tn+fp)
      
    # Dict. of Evaluation
    scoring = {'accuracy': accuracy_score(y_test, y_pred),
               'precision':metrics.precision_score(y_test, y_pred, average='macro'),
               'specificity':specificity,
               'f1score':metrics.f1_score(y_test, y_pred, average='macro') ,
               'sensitivity':metrics.recall_score(y_test, y_pred, average='macro'),
               'Time':speed}
    
    return scoring

"""##callLGBM"""

def callLGBM(max_depth, iteration, learning_rate, 
            num_leaves, noTree, min_data_in_leaf, data, useKfolds):
  """
    - Input: parameters needed for LGBM
    
    - Output: performance metrics
     - Accuracy
     - Prediction
     - Specificity
     - F1 Score
     - Sensitivity
     - Time
  """
  if useKfolds >1:
    accuracy=[]
    precision=[]
    specificity=[]
    F1Score=[]
    Sensitivity=[]
    Time=[]

    for i in range(useKfolds):
      #) Receive the "data" 
      X_train = pd.DataFrame(data[i][0])
      X_test = pd.DataFrame(data[i][1])
      y_train = pd.DataFrame(data[i][2])
      y_test = pd.DataFrame(data[i][3])

      #convert to LGBM compatible dataset
      d_train = lgb.Dataset(X_train, label=y_train)
      d_test = lgb.Dataset(X_test, label=y_test)

      t0 = time.perf_counter()

      #*/ IF USING SINGLE USER-DEFINED PARAMETERS
      #) Configure Parameters for XGBoost
      # specify parameters via map or direct assignment

      #assigning the paramters of the lgbm 
      param = {}
      #Hardcoded
      param['boosting_type'] = 'gbdt'
      param['sub_feature'] = 0.5
      
      #Parameters passed from user
      param['num_leaves'] = num_leaves
      param['min_data'] = min_data_in_leaf
      param['max_depth'] = max_depth  
      param['learning_rate'] = learning_rate 

      #Parameters Dependent on the dataset
      #'objective': 'binary:hinge' #'multi:softprob',  # 'binary:logistic'
      if len(y_train[0].unique()) == 2:
        target_type = 'binary'
        param['objective'] = target_type 
     
        param['metric'] = 'binary_logloss'
      elif len(y_train[0].unique()) > 2:
        target_type = 'multiclass'
        param['objective'] = target_type
        param['metric'] = 'multi_logloss'
        param['num_class'] = len(y_train[0].unique())
    

      #/* END IF USING SINGLE PARAMETERS

      # Fitting classifier to the Training set
      # Training the model
      clf = lgb.train(param, d_train, iteration)

      #) Predict testing data classes, store best predictions in array
      if target_type == 'binary':
        y_pred = clf.predict(X_test)
        for idx,val in enumerate(y_pred):
          if y_pred[idx] >= .5:
            y_pred[idx] = 1
          else:
            y_pred[idx] = 0
      elif target_type == 'multiclass':
        y_pred = clf.predict(X_test)
        y_pred = np.asarray([np.argmax(line) for line in y_pred])

      # Speed
      speed =np.around(time.perf_counter()-t0, decimals=3)
      
      # confusion_matrix
      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel() #for binary class only
      specificity_fold = tn/(tn+fp)
      
      # evaluation
      accuracy.append(accuracy_score(y_test, y_pred))
      precision.append(metrics.precision_score(y_test, y_pred, average='macro'))
      specificity.append(specificity_fold)
      F1Score.append(metrics.f1_score(y_test, y_pred, average='macro'))
      Sensitivity.append(metrics.recall_score(y_test, y_pred, average='macro'))
      Time.append(speed)
        
    scoring = {'accuracy':np.mean(accuracy),
               'precision':np.mean(precision),
               'specificity':np.mean(specificity),
               'f1score':np.mean(F1Score),
               'sensitivity':np.mean(Sensitivity),
               'Time':np.mean(speed)}

    return scoring

  else:
    X_train = pd.DataFrame(data[0])
    X_test = pd.DataFrame(data[1])
    y_train = pd.DataFrame(data[2])
    y_test = pd.DataFrame(data[3])
    
    #convert to LGBM compatible dataset
    d_train = lgb.Dataset(X_train, label=y_train)
    d_test = lgb.Dataset(X_test, label=y_test)
    
    t0 = time.perf_counter()

    #assigning the paramters of the lgbm 
    param = {}
    #Hardcoded
    param['boosting_type'] = 'gbdt'
    param['sub_feature'] = 0.5
    
    #Parameters passed from user
    
    param['num_leaves'] = num_leaves
    param['min_data'] = min_data_in_leaf
    param['max_depth'] = max_depth 
    param['learning_rate'] = learning_rate 

    
    if len(y_train[0].unique()) == 2:
      target_type = 'binary'
      param['objective'] = target_type 
      param['metric'] = 'binary_logloss'
    elif len(y_train[0].unique()) > 2:
      target_type = 'multiclass'
      param['objective'] = target_type
      param['metric'] = 'multi_logloss'
      param['num_class'] = len(y_train[0].unique()) 


    #/* END IF USING SINGLE PARAMETERS

    # Fitting classifier to the Training set
    clf = lgb.train(param, d_train, iteration)

    #) Predict testing data classes, store best predictions in array
    if target_type == 'binary':
      y_pred = clf.predict(X_test)
      for idx,val in enumerate(y_pred):
        if y_pred[idx] >= .5:
          y_pred[idx] = 1
        else:
          y_pred[idx] = 0
    elif target_type == 'multiclass':
      y_pred = clf.predict(X_test)
      y_pred = np.asarray([np.argmax(line) for line in y_pred]) 

    # Speed
    speed =np.around(time.perf_counter()-t0, decimals=3)

    # confusion_matrix   
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    specificity = tn/(tn+fp)
    
    #Dict. of Evaluation
    scoring = {'accuracy': accuracy_score(y_test, y_pred),
               'precision':metrics.precision_score(y_test, y_pred, average='macro'),
               'specificity':specificity,
               'f1score':metrics.f1_score(y_test, y_pred, average='macro') ,
               'sensitivity':metrics.recall_score(y_test, y_pred, average='macro'),
               'Time':speed}

    
    return scoring

"""##callSKGBDT"""

from sklearn.ensemble import GradientBoostingClassifier
def callSKGBDT(max_depth,n_estimators,max_leaf_nodes, data,useKfolds,learning_rate):

  """
  - Input: parameters needed for SKGBDT
  - Output: performance metrics
   - Accuracy
   - Prediction
   - Specificity
   - F1 Score
   - Sensitivity
   - Time
  """
  
  #Kfolds method
  if useKfolds >1:
    accuracy=[]
    precision=[]
    specificity=[]
    F1Score=[]
    Sensitivity=[]
    Time=[]
    
    for i in range(useKfolds):
      X_train = data[i][0]
      X_test = data[i][1]
      y_train = data[i][2]
      y_test = data[i][3]
      
      t0 = time.perf_counter()
          
      
      # Fitting classifier to the Training set
      classifier = GradientBoostingClassifier(n_estimators = n_estimators, max_depth = max_depth,max_leaf_nodes = max_leaf_nodes, random_state = 0,learning_rate = learning_rate)
      classifier.fit(X_train, y_train)

      # Predicting the Test set results
      y_pred = classifier.predict(X_test)

      # Speed
      speed =np.around(time.perf_counter()-t0, decimals=3)

      # confusion_matrix
      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
      specificity_fold = tn/(tn+fp)
      
      # evaluation
      accuracy.append(accuracy_score(y_test, y_pred))
      precision.append(metrics.precision_score(y_test, y_pred, average='macro'))
      specificity.append(specificity_fold)

      F1Score.append(metrics.f1_score(y_test, y_pred, average='macro'))
      Sensitivity.append(metrics.recall_score(y_test, y_pred, average='macro'))
      Time.append(speed)



    scoring = {'accuracy':np.mean(accuracy),
               'precision':np.mean(precision),
               'specificity':np.mean(specificity),
               'f1score':np.mean(F1Score),
               'sensitivity':np.mean(Sensitivity),
               'Time':np.mean(speed)}
    return scoring
  
  #Split method
  else:
    X_train = data[0]
    X_test = data[1]
    y_train = data[2]
    y_test = data[3]
    
    t0 = time.perf_counter()
    
    # Fitting Random Forest to the Training set
    classifier = GradientBoostingClassifier(n_estimators = n_estimators, max_depth = max_depth,max_leaf_nodes = max_leaf_nodes, random_state = 0,learning_rate = learning_rate )
    classifier.fit(X_train, y_train)

    # Predicting the Test set results
    y_pred = classifier.predict(X_test)
    
    
    # Speed
    speed =np.around(time.perf_counter()-t0, decimals=3)

    # confusion_matrix
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    specificity = tn/(tn+fp)
    
    #Dict. of Evaluation

    scoring = {'accuracy': accuracy_score(y_test, y_pred),
               'precision':metrics.precision_score(y_test, y_pred, average='macro'),
               'specificity':specificity,
               'f1score':metrics.f1_score(y_test, y_pred, average='macro') ,
               'sensitivity':metrics.recall_score(y_test, y_pred, average='macro'),
               'Time':speed}


    return scoring

"""##callSKRF"""

from sklearn.ensemble import RandomForestClassifier
def callSKRF(max_depth,n_estimators,max_leaf_nodes, data,useKfolds):   
    
    """
    - Input: parameters needed for XGBoost
    
    - Output: performance metrics
     - Accuracy
     - Prediction
     - Specificity
     - F1 Score
     - Sensitivity
     - Time
    """
    if useKfolds >1:
        
        accuracy=[]
        precision=[]
        specificity=[]
        F1Score=[]
        Sensitivity=[]
        Time=[]
        
        for i in range(useKfolds): 
            X_train = data[i][0]
            X_test = data[i][1]
            y_train = data[i][2]
            y_test = data[i][3]
    
            t0 = time.perf_counter()
            # Fitting Random Forest to the Training set
            classifier = RandomForestClassifier(n_estimators = n_estimators,max_depth = max_depth, max_leaf_nodes = max_leaf_nodes, criterion = 'entropy', random_state = 0 )
            classifier.fit(X_train, y_train)
    
            # Predicting the Test set results
            y_pred = classifier.predict(X_test)
        
            # Speed
            speed =np.around(time.perf_counter()-t0, decimals=3)
            
            # confusion_matrix
            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
            specificity_fold = tn/(tn+fp)
            # evaluation
            accuracy.append(accuracy_score(y_test, y_pred))
            precision.append(metrics.precision_score(y_test, y_pred, average='macro'))
            specificity.append(specificity_fold)
            F1Score.append(metrics.f1_score(y_test, y_pred, average='macro'))
            Sensitivity.append(metrics.recall_score(y_test, y_pred, average='macro'))
            Time.append(speed)
        
        
        
        scoring = {'accuracy':np.mean(accuracy) ,
               'precision':np.mean(precision),
               'specificity':np.mean(specificity),
               'f1score':np.mean(F1Score),
               'sensitivity':np.mean(Sensitivity),
               'Time':np.mean(speed)}
        return scoring
    
    else: 
        X_train = data[0]
        X_test = data[1]
        y_train = data[2]
        y_test = data[3]
    
        t0 = time.perf_counter()
        # Fitting Random Forest to the Training set
        classifier = RandomForestClassifier(n_estimators = n_estimators,max_depth = max_depth, max_leaf_nodes = max_leaf_nodes, criterion = 'entropy', random_state = 0 )
        classifier.fit(X_train, y_train)
    
        # Predicting the Test set results
        y_pred = classifier.predict(X_test)
        
        # Speed
        speed =np.around(time.perf_counter()-t0, decimals=3)
    
        # confusion_matrix
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        specificity = tn/(tn+fp)
    
        #Dict. of Evaluation

        scoring = {'accuracy': accuracy_score(y_test, y_pred),
                   'precision':metrics.precision_score(y_test, y_pred, average='macro'),
                   'specificity':specificity,
                   'f1score':metrics.f1_score(y_test, y_pred, average='macro'),
                   'sensitivity':metrics.recall_score(y_test, y_pred, average='macro'),
                   'Time':speed}
        
        return scoring

"""##GridSearch

###CalculateOptimalParameter
"""

def CalculateOptimalParameter(data, useKfolds, useGridSearch):
    """  
    
    Inputs : same as the parameter abstraction function
    ---outputXGboost = carry out the Gridsearch for XGBoost algorithm
    ---outputLGBM = carry out the Gridsearch for LGBM algorithm
    ---outputSKRF =carry out the Gridsearch for Random Forest algorithm
    ---outputSKGBDT =carry out the Gridsearch for SKGBDT algorithm
    
    Output : 
        Dictionary of the Outputs
          output = {
        "XGBoost": outputXGboost,
        "LGBM": outputLGBM,
        "SKGBDT": outputSKGBDT,
        "SKRF": outputSKRF
    }
    """
    
    outputXGboost = {
       'accuracy':0,
       'precision':0,
       'specificity':0,
       'f1score':0,
       'sensitivity':0,
       'Time':0
    }
    outputLGBM = {
       'accuracy':0,
       'precision':0,
       'specificity':0,
       'f1score':0,
       'sensitivity':0,
       'Time':0
    }
    outputSKGBDT = {
       'accuracy':0,
       'precision':0,
       'specificity':0,
       'f1score':0,
       'sensitivity':0,
       'Time':0
    }
    outputSKRF = {
       'accuracy':0,
       'precision':0,
       'specificity':0,
       'f1score':0,
       'sensitivity':0,
       'Time':0
    }
    

    outputXGboost = callXGBoostGS(data=data, useKfolds=useKfolds)
    outputLGBM = callLGBMGS(data=data, useKfolds=useKfolds)
    outputSKGBDT = callDTGS(data=data, useKfolds=useKfolds)
    outputSKRF = callRFGS(data=data, useKfolds=useKfolds)
    
    output = {
        "XGBoost": outputXGboost,
        "LGBM": outputLGBM,
        "SKGBDT": outputSKGBDT,
        "SKRF": outputSKRF
    }
    
    return output

"""###XGB"""

def callXGBoostGS(data, useKfolds):
  """
  - Input: parameters needed for XGBoost

  - Output: performance metrics
   - Accuracy
   - Prediction
   - Specificity
   - F1 Score
   - Sensitivity
   - Time
  """
  
  X_train = pd.DataFrame(data[0])
  X_test = pd.DataFrame(data[1])
  y_train = pd.DataFrame(data[2])
  y_test = pd.DataFrame(data[3])

  
  #*/ IF USING GRIDSEARCH
  #) Configure Parameters for XGBoost
  #For GridSearch
  params_fixed = {
      'silent': 1 #, # logging mode - quiet
  }
  
  params_grid = {
      'learning_rate': [0.1, 0.2,0.3], # the training step for each iteration
      'n_estimators':[5, 10], #'num_boost_round': 5 #same as n_estimators, num_round
      'max_depth':[3,5,10], # the maximum depth of each tree 
  }

  if len(y_train[0].unique()) == 2:
    target_type = 'binary:logistic' #'binary:hinge'
    params_fixed['objective'] = target_type
  elif len(y_train[0].unique())> 2:
    target_type = 'multi:softprob'
    params_fixed['objective'] = target_type
    params_fixed['num_class'] = len(y_train[0].unique()) # the number of classes that exist in this dataset

  #/* END IF USING GRIDSEARCH PARAMETERS

  if useKfolds <= 1:
    cv = 10
  else:
    cv=useKfolds
  
  #) Apply GridSearch
  bst_grid = GridSearchCV(estimator=xgb.XGBClassifier(**params_fixed, seed=925),param_grid=params_grid,cv=cv,scoring='accuracy',iid=False)#, verbose=10)
  bst_grid.fit(X_train, y_train)

  paramgs = {
      'max_depth': bst_grid.best_params_['max_depth'],  # the maximum depth of each tree #SOME FINE-TUNING MAY BE NEEDED?
      'eta': bst_grid.best_params_['learning_rate'],  # the training step for each iteration
      'maxLeafnode': 0,
      'ntree_limit': 0,
      'min_child_weight': 0,
      'num_boost_round': bst_grid.best_params_['n_estimators'] #same as n_estimators
  }
  
  scoring = callXGBoost(**paramgs, data=data, useKfolds=0) #, useKfolds=useKfolds)
  
  return scoring

"""###LGBM"""

def callLGBMGS(data, useKfolds):
  """
  - Input: parameters needed for XGBoost

  - Output: performance metrics
   - Accuracy
   - Prediction
   - Specificity
   - F1 Score
   - Sensitivity
   - Time
  """
  
  X_train = pd.DataFrame(data[0])
  X_test = pd.DataFrame(data[1])
  y_train = pd.DataFrame(data[2])
  y_test = pd.DataFrame(data[3])

  
  #*/ IF USING GRIDSEARCH
  #) Configure Parameters for LGBM
  #For GridSearch
  params_fixed = {
      'silent': 1 #, # logging mode - quiet
  }
  
  params_grid = {
      'learning_rate': [0.1, 0.2,0.3], # the training step for each iteration
      'n_estimators':[5, 10], #'num_boost_round': 5 #same as n_estimators, num_round
      'max_depth':[3, 5,10], # the maximum depth of each tree 
  }

  #'objective': 'binary:hinge'
  if len(y_train[0].unique()) == 2:
    target_type = 'binary:logistic' #'binary:hinge'
    params_fixed['objective'] = target_type
  elif len(y_train[0].unique())> 2:
    target_type = 'multi:softprob'
    params_fixed['objective'] = target_type
    params_fixed['num_class'] = len(y_train[0].unique()) # the number of classes that exist in this dataset
  
  #/* END IF USING GRIDSEARCH PARAMETERS

  if useKfolds <= 1:
    cv = 10
  else:
    cv=useKfolds
  
  #) Apply GridSearch
  bst_grid = GridSearchCV(estimator=xgb.XGBClassifier(**params_fixed, seed=925),param_grid=params_grid,cv=cv,scoring='accuracy',iid=False)#, verbose=10)
  bst_grid.fit(X_train, y_train)

  paramgs = {
      'max_depth': bst_grid.best_params_['max_depth'],  # the maximum depth of each tree #SOME FINE-TUNING MAY BE NEEDED?
      'learning_rate': bst_grid.best_params_['learning_rate'],  # the training step for each iteration
      'num_leaves': 2,
      'noTree': 0,
      'min_data_in_leaf': 0,
      'iteration': bst_grid.best_params_['n_estimators'] #same as n_estimators
  }
  
  scoring = callLGBM(**paramgs, data=data, useKfolds=0) #, useKfolds=useKfolds)
  
  return scoring

"""###RF"""

def callRFGS(data, useKfolds):

  
  X_train = pd.DataFrame(data[0])
  X_test = pd.DataFrame(data[1])
  y_train = pd.DataFrame(data[2])
  y_test = pd.DataFrame(data[3])
  
  #For GridSearch
  params_fixed = {
      #'silent': 1 #, # logging mode - quiet
  }
  
  params_grid = {
      'n_estimators':[5, 10], #'num_boost_round': 5 #same as n_estimators, num_round
      'max_depth':[3, 5,10], # the maximum depth of each tree         
  }

  #/* END IF USING GRIDSEARCH PARAMETERS

  if useKfolds <= 1:
    cv = 10
  else:
    cv=useKfolds
  
  #) Apply GridSearch
  bst_grid = GridSearchCV(estimator=RandomForestClassifier(**params_fixed),param_grid=params_grid,cv=cv,scoring='accuracy',iid=False)#, verbose=10)
  bst_grid.fit(X_train, y_train)

  paramgs = {
      'max_depth': bst_grid.best_params_['max_depth'], 
      'max_leaf_nodes':2,
      'n_estimators': bst_grid.best_params_['n_estimators']
  }
  
  scoring = callSKRF(**paramgs, data=data, useKfolds=0) #, useKfolds=useKfolds)
  
  return scoring

"""###DT"""

def callDTGS(data, useKfolds):

  
  X_train = pd.DataFrame(data[0])
  X_test = pd.DataFrame(data[1])
  y_train = pd.DataFrame(data[2])
  y_test = pd.DataFrame(data[3])
  
  #For GridSearch
  params_fixed = {
      #'silent': 1 #, # logging mode - quiet
  }
  
  params_grid = {
      'learning_rate': [0.1, 0.2,0.3], 
      'n_estimators':[5, 10], 
      'max_depth':[3, 5,10],      
  }

  #/* END IF USING GRIDSEARCH PARAMETERS

  if useKfolds <= 1:
    cv = 10
  else:
    cv=useKfolds
  
  #) Apply GridSearch
  bst_grid = GridSearchCV(estimator=GradientBoostingClassifier(**params_fixed),param_grid=params_grid,cv=cv,scoring='accuracy',iid=False)#, verbose=10)
  bst_grid.fit(X_train, y_train)

  paramgs = {
      'max_depth': bst_grid.best_params_['max_depth'],  # the maximum depth of each tree #SOME FINE-TUNING MAY BE NEEDED?
      'learning_rate': bst_grid.best_params_['learning_rate'],  # the training step for each iteration
      'max_leaf_nodes':2,
      'n_estimators': bst_grid.best_params_['n_estimators'] #same as n_estimators
  }
  
  scoring = callSKGBDT(**paramgs, data=data, useKfolds=0) #, useKfolds=useKfolds)
  
  return scoring

"""# calculateRank"""

def calculateRank(metrics, result):
    """
    calculate and get optimal library for each dataset and based on user-set weights
    
    - Input 
      - metrics: output from callLibrary
        (performance metrics for each library)
      - weights: user-defined weights
      
    - Output
      - ranks: sorted order of libraried based on weighted sum
    """
    weighted_score = {}
    for key, value in metrics.items():
             
        weighted_score[key] = result["precision"] * value["precision"] + \
                              result["accuracy"] * value["accuracy"]  + \
                              result["specificity"] * value["specificity"] + \
                              result["f1score"] * value["f1score"] + \
                              result["sensitivity"] * value["sensitivity"]
    ranks = dict(sorted(weighted_score.items(), key=lambda x: x[1]))
    return ranks

"""##getInput"""

def getInput(loc,labelName,accuracyWeight,precisionWeight,specificityWeight,f1scoreWeight,sensitivityWeight):
    """
    get dataset and parameters setting, weights from interface
    """
    inputs = {}
    
    inputs["loc"]= loc
    inputs["labelName"]=labelName
    
    inputs["centralValue"]= 'mean'
    inputs["normalizationType"]= 'minmax'
    inputs["useKfolds"]= 10
    inputs["trainSize"]= 0.7
    inputs["learningRate"]= 0.3
    inputs["iteration"]= 5
    inputs["maxLeafnode"]= 2 #None
    inputs["noTree"]= 5
    inputs["minData"]= None
    inputs["maxDepth"]= 3
    inputs["useGPU"]= False
    
    inputs["precision"]= precisionWeight
    inputs["accuracy"]= accuracyWeight
    inputs["specificity"]= specificityWeight
    inputs["f1score"]= f1scoreWeight
    inputs["sensitivity"]= sensitivityWeight
    inputs["useGridSearch"]= 0
    return inputs

"""## Call the functions"""

warnings.filterwarnings('ignore')

"""
Main Function
"""
def __main__(loc,labelName,accuracyWeight,precisionWeight,specificityWeight,f1scoreWeight,sensitivityWeight):
    #Get the Inputs (parameters, metric weights)
    result = getInput(loc,labelName,accuracyWeight,precisionWeight,specificityWeight,f1scoreWeight,sensitivityWeight)
    
    #Data Preprocessing Step
    dataOutput = dataPreprocess(result["loc"], result["labelName"], result["centralValue"], 
                   result["normalizationType"], result["useKfolds"], 
                   result["trainSize"], result["useGridSearch"])
    
    #Parameter Abstraction step
    if (result['useGridSearch'] == 1):
      #print ('Use Grid Search = True')
      metricsOutput = CalculateOptimalParameter(dataOutput, result["useKfolds"], result["useGridSearch"])
    else:
      #print ('Use Grid Search = False')
      parameter_output = parameterAbstract(result["maxDepth"], result["iteration"], result["learningRate"], result["maxLeafnode"], result["noTree"], result["minData"], result["useGPU"])
      metricsOutput = callLibrary(parameter_output, dataOutput, result["useKfolds"], result["useGridSearch"])
      
    
    rank_output = calculateRank(metricsOutput,result)
    print("Dataset: ", loc, " Label:", labelName)
    print("MetricsOutput",metricsOutput)
    print("Ranking" , rank_output)
    print("\n")
    
    return metricsOutput

"""## Plot the results"""

"""
Function to make the radar charts
"""

def makeRadarChart(metricsOutput):
  del metricsOutput["LGBM"]["Time"]
  del metricsOutput["XGBoost"]["Time"]
  del metricsOutput["SKGBDT"]["Time"]
  del metricsOutput["SKRF"]["Time"]

  data = [
      go.Scatterpolar(
        r = list(metricsOutput["LGBM"].values()),
        theta = list(metricsOutput["LGBM"].keys()),
        name = 'LGBM',
        fill = 'toself'
      ),
     go.Scatterpolar(
        r = list(metricsOutput["XGBoost"].values()),
        theta = list(metricsOutput["XGBoost"].keys()),
        name = 'XGBoost',
        fill = 'toself'
      ),
       go.Scatterpolar(
        r = list(metricsOutput["SKGBDT"].values()),
        theta = list(metricsOutput["SKGBDT"].keys()),
        name = 'SKGBDT',
        fill = 'toself'
      ),
       go.Scatterpolar(
        r = list(metricsOutput["SKRF"].values()),
        theta = list(metricsOutput["SKRF"].keys()),
        name = 'SKRF',
        fill = 'toself'
      )
  ]
  layout = go.Layout(
    polar = dict(
      radialaxis = dict(
        visible = True,
        range = [0, 1],
         
      )
    ),
    showlegend = True
  )

  fig = go.Figure(data=data, layout=layout)
  return plotly.plotly.iplot(fig)

"""##Scenario 1
Accuracy Weight = 1 <br>
Precision Weight = 0 <br>
Speficity Weight = 0  <br>
F1Score Weight = 0  <br>
Sensitivity Weight = 0  <br>
"""

out = __main__('/content/Social_Network_Ads.csv','Purchased',0,1,0,0,0)
makeRadarChart(out)

"""##Scenario 2
Accuracy Weight = 0 <br>
Precision Weight = 1 <br>
Speficity Weight = 0  <br>
F1Score Weight = 0  <br>
Sensitivity Weight = 0  <br>
"""

out = __main__('/content/Social_Network_Ads.csv','Purchased',0,1,0,0,0)
makeRadarChart(out)

"""##Scenario 3
Accuracy Weight = 0<br>
Precision Weight = 0 <br>
Speficity Weight = 1  <br>
F1Score Weight = 0  <br>
Sensitivity Weight = 0  <br>
"""

out = __main__('/content/Social_Network_Ads.csv','Purchased',0,0,1,0,0)
makeRadarChart(out)

"""##Scenario 4
Accuracy Weight = 0<br>
Precision Weight = 0 <br>
Speficity Weight = 0 <br>
F1Score Weight = 1  <br>
Sensitivity Weight = 0  <br>
"""

out = __main__('/content/Social_Network_Ads.csv','Purchased',0,0,0,1,0)
makeRadarChart(out)

"""##Scenario 5
Accuracy Weight = 0<br>
Precision Weight = 0 <br>
Speficity Weight = 0 <br>
F1Score Weight = 0  <br>
Sensitivity Weight = 1  <br>
"""

out = __main__('/content/Social_Network_Ads.csv','Purchased',0,0,0,0,1)
makeRadarChart(out)